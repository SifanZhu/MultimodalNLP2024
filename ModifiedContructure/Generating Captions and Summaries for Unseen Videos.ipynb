{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-Tuning the BLIP Model\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Initialize the processor and model for BLIP\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model_img_captioning = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cpu\")\n",
    "\n",
    "# Define the dataset class\n",
    "class AnnotatedFrameDataset(Dataset):\n",
    "    def __init__(self, annotations_file, processor):\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            self.annotations = json.load(f)[\"videos\"]\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(video[\"frames\"]) for video in self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_idx, frame_idx = 0, idx\n",
    "        while frame_idx >= len(self.annotations[video_idx][\"frames\"]):\n",
    "            frame_idx -= len(self.annotations[video_idx][\"frames\"])\n",
    "            video_idx += 1\n",
    "\n",
    "        frame_info = self.annotations[video_idx][\"frames\"][frame_idx]\n",
    "        image = Image.open(frame_info[\"frame_file\"]).convert(\"RGB\")\n",
    "        description = frame_info[\"annotations\"][\"description\"]\n",
    "        inputs = self.processor(images=image, text=description, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        return inputs.input_ids.squeeze(), inputs.attention_mask.squeeze(), inputs.labels.squeeze()\n",
    "\n",
    "# Load the dataset\n",
    "dataset = AnnotatedFrameDataset(\"annotations.json\", processor)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tuning loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_img_captioning.to(device)\n",
    "optimizer = torch.optim.AdamW(model_img_captioning.parameters(), lr=5e-5)\n",
    "\n",
    "model_img_captioning.train()\n",
    "for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        outputs = model_img_captioning(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "print(\"Fine-tuning complete.\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_img_captioning.save_pretrained(\"path_to_save_fine_tuned_model\")\n",
    "processor.save_pretrained(\"path_to_save_fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Captions and Summaries for Unseen Videos\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"path_to_save_fine_tuned_model\")\n",
    "model_img_captioning = BlipForConditionalGeneration.from_pretrained(\"path_to_save_fine_tuned_model\").to(\"cpu\")\n",
    "\n",
    "# Initialize the tokenizer and model for summarization\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(\"cpu\")\n",
    "\n",
    "def generate_caption(pil_image):\n",
    "    inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}\n",
    "    out = model_img_captioning.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Define paths\n",
    "video_folder_path = '/Users/kristinakuznetsova/Downloads/untitledfolder'\n",
    "frames_folder = \"/Users/kristinakuznetsova/Downloads/frames\"\n",
    "os.makedirs(frames_folder, exist_ok=True)\n",
    "\n",
    "# Process each video\n",
    "video_files = [f for f in os.listdir(video_folder_path) if f.endswith(('mp4', 'avi', 'mkv'))]\n",
    "all_video_captions = []\n",
    "\n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(video_folder_path, video_file)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_captions = []\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    sample_interval = int(frame_rate)  # sample one frame per second\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % sample_interval == 0:\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            caption = generate_caption(pil_image)\n",
    "            video_captions.append(caption)\n",
    "\n",
    "            # Save the frame\n",
    "            frame_filename = f\"{os.path.splitext(video_file)[0]}_{frame_count}.jpg\"\n",
    "            frame_path = os.path.join(frames_folder, frame_filename)\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if video_captions:\n",
    "        input_text = \"summarize: \" + \" \".join(video_captions)\n",
    "        inputs = tokenizer_t5(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        summary_ids = model_t5.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer_t5.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        all_video_captions.append(f\"{video_file}: {summary}\")\n",
    "\n",
    "print(\"\\nAll video summaries:\")\n",
    "for video_summary in all_video_captions:\n",
    "    print(video_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
